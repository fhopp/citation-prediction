{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAWING\n",
      "WRITING\n",
      "PLOTTING\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "\n",
    "randomList = random.sample(range(1, 504), 304)\n",
    "\n",
    "path_author_ids = \"data/author_ids.txt\"\n",
    "\n",
    "f = open(path_author_ids, 'r', encoding=\"Latin-1\")\n",
    "author_ids = {}\n",
    "for line in f:\n",
    "    line_content = line.split(\"\\n\")\n",
    "    id, key = line_content[0].split(\"\\t\")\n",
    "    author_ids[key] = id\n",
    "\n",
    "\n",
    "# Create the dict of authors with citations\n",
    "\n",
    "path_author_citations = \"data/author_citations.txt\"\n",
    "\n",
    "f = open(path_author_citations, 'r', encoding=\"Latin-1\")\n",
    "author_citations = {}\n",
    "for line in f:\n",
    "    line_content = line.split(\"\\n\")\n",
    "    citation_count, author = line_content[0].split(\"\\t\")\n",
    "    if author in author_ids.keys():\n",
    "        author_citations[author] = int(citation_count)\n",
    "\n",
    "# Paper and citation counts\n",
    "\n",
    "path_paper_citations = \"data/paper_citations.txt\"\n",
    "\n",
    "f = open(path_paper_citations, 'r', encoding=\"Latin-1\")\n",
    "paper_citations = {}\n",
    "for line in f:\n",
    "    line_content = line.split(\"\\n\")\n",
    "    splitedParts = line_content[0].split(\"\\t\")\n",
    "    citation_count, paper = splitedParts[0], splitedParts[1]\n",
    "    paper_citations[paper] = int(citation_count)\n",
    "\n",
    "    \n",
    "# Paper and authors\n",
    "\n",
    "path_paper_authors = \"data/paper_author_affiliations.txt\"\n",
    "\n",
    "f = open(path_paper_authors, 'r', encoding=\"Latin-1\")\n",
    "paper_authors = {}\n",
    "for line in f:\n",
    "    line_content = line.split(\"\\n\")\n",
    "    paper, authors = line_content[0].split(\"\\t\")\n",
    "    authorlist = authors.split(\";\")\n",
    "    paper_authors[paper] = authorlist\n",
    "    \n",
    "\n",
    " \n",
    "# TOP 100 authors in terms of citation count\n",
    "top100_sorted_author_citations = dict(sorted(author_citations.items(), key=operator.itemgetter(1), reverse=True)[:100])\n",
    "sorted_author_citations = dict(sorted(author_citations.items(), key=operator.itemgetter(1), reverse=True))\n",
    "#sorted_author_citations = sorted(author_citations.items(), key=operator.itemgetter(1))\n",
    "#top100_author_citations = sorted_author_citations[len(author_citations.items())-100:len(author_citations.items())]\n",
    "#print(sorted_author_citations.keys())\n",
    "    \n",
    "path_author_collaboration_network = \"data/author-collaboration-network.txt\"\n",
    "#path_author_collaboration_network = \"data/test.txt\"\n",
    "\n",
    "f = open(path_author_collaboration_network, 'r', encoding=\"Latin-1\")\n",
    "\n",
    "author_collaboration_network = nx.Graph()\n",
    "\n",
    "for v in author_citations.keys():\n",
    "    author_collaboration_network.add_node(v)\n",
    "\n",
    "for line in f:\n",
    "    sumcitation = 0\n",
    "    line_content = line.split(\"\\n\")\n",
    "    name_author, name_coauthor = line_content[0].split(\"==>\")\n",
    "    #if name_author in top100_sorted_author_citations.keys() and name_coauthor in top100_sorted_author_citations.keys():\n",
    "    #    author_collaboration_network.add_edge(name_author, name_coauthor)\n",
    "    \n",
    "    if name_author in author_ids.keys() and name_coauthor in author_ids.keys():\n",
    "        for key, value in paper_authors.items():\n",
    "            if author_ids[name_author] in value and author_ids[name_coauthor] in value :\n",
    "                if key in paper_citations.keys():\n",
    "                    sumcitation = sumcitation + paper_citations[key]\n",
    "                #print (name_author + \" & \" + name_coauthor + \" --> \" + sumcitation)\n",
    "        author_collaboration_network.add_edge(name_author, name_coauthor, weight=sumcitation)\n",
    "        #if sumcitation>50:\n",
    "        #    print (name_author + \" & \" + name_coauthor + \" --> \" + str(sumcitation))\n",
    "    \n",
    "f.close()\n",
    "print(\"DRAWING\")\n",
    "# UNCOMMENT ALL\n",
    "#try:\n",
    "#    pos=nx.graphviz_layout(author_collaboration_network)\n",
    "#except:\n",
    "#    pos = nx.spring_layout(author_collaboration_network, iterations=20)\n",
    "\n",
    "\n",
    "#plt.figure(1,figsize=(30,30))\n",
    "#nx.draw(author_collaboration_network, pos, node_size=[author_citations[v] for v in author_collaboration_network.nodes()], with_labels=True)\n",
    "#plt.show()  # display\n",
    "\n",
    "#print(sorted(top100_sorted_author_citations.items(), key=operator.itemgetter(1), reverse=True)[:100])\n",
    "\n",
    "#print(sum(top100_sorted_author_citations.values())/sum(sorted_author_citations.values()))\n",
    "\n",
    "#print(sorted(nx.load_centrality(author_collaboration_network).items(), key=operator.itemgetter(1), reverse=True)[:100])\n",
    "\n",
    "print(\"WRITING\")\n",
    "\n",
    "#outputfile_centrality_eigen = open('data/author_eigenvector_centrality_weighted.txt', 'w', encoding=\"Latin-1\")\n",
    "\n",
    "#for item in sorted(nx.eigenvector_centrality(author_collaboration_network, weight = 'weight').items(), key=operator.itemgetter(1), reverse=True):\n",
    "#    outputfile_centrality_eigen.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "#outputfile_centrality_eigen.close()\n",
    "\n",
    "#outputfile_centrality_load = open('data/author_load_centrality_weighted.txt', 'w', encoding=\"Latin-1\")\n",
    "#for item in sorted(nx.load_centrality(author_collaboration_network, weight = 'weight').items(), key=operator.itemgetter(1), reverse=True):\n",
    "#    outputfile_centrality_load.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "#outputfile_centrality_load.close()\n",
    "    \n",
    "#outputfile_sorted_citations = open('data/author_citations_sorted.txt', 'w', encoding=\"Latin-1\")\n",
    "#for item in sorted(author_citations.items(), key=operator.itemgetter(1), reverse=True):\n",
    "#    outputfile_sorted_citations.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "\n",
    "\n",
    "print(\"PLOTTING\")\n",
    "\n",
    "#print(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "\n",
    "#keys = list(sorted_author_citations.keys())\n",
    "#values = sorted(sorted_author_citations.values(), reverse=True)\n",
    "\n",
    "#x = range(len(keys))\n",
    "#plt.figure(1,figsize=(10,10))\n",
    "#plt.scatter(x, values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputfile_centrality_eigen = open('data/author_eigenvector_centrality_weighted.txt', 'w', encoding=\"Latin-1\")\n",
    "\n",
    "for item in sorted(nx.eigenvector_centrality(author_collaboration_network, weight = 'weight').items(), key=operator.itemgetter(1), reverse=True):\n",
    "    outputfile_centrality_eigen.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "outputfile_centrality_eigen.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_dict = {}\n",
    "for key, value in nx.eigenvector_centrality(author_collaboration_network, weight = 'weight').items():\n",
    "    if key in author_citations.keys():\n",
    "        new_value = value*author_citations[key]\n",
    "        mixed_dict[key] = new_value\n",
    "        \n",
    "outputfile_centrality_mix = open('data/author_citation_centrality_mixed.txt', 'w', encoding=\"Latin-1\")\n",
    "\n",
    "for item in sorted(mixed_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    outputfile_centrality_mix.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "outputfile_centrality_mix.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.077287523238359\n"
     ]
    }
   ],
   "source": [
    "isitone = 0\n",
    "for key, value in nx.eigenvector_centrality(author_collaboration_network, weight = 'weight').items():\n",
    "    isitone = isitone + value\n",
    "    \n",
    "print(isitone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.607386859476286\n"
     ]
    }
   ],
   "source": [
    "isitone = 0\n",
    "for key, value in nx.eigenvector_centrality(author_collaboration_network, weight = None).items():\n",
    "    isitone = isitone + value\n",
    "    \n",
    "print(isitone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in [2008,2009,2011,2012]:\n",
    "#for year in [2013]:   \n",
    "    # Create the dict of authors with citations\n",
    "\n",
    "    path_author_citations = \"data/\" + str(year) + \"/author_citations.txt\"\n",
    "\n",
    "    f = open(path_author_citations, 'r', encoding=\"Latin-1\")\n",
    "    author_citations = {}\n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        citation_count, author = line_content[0].split(\"\\t\")\n",
    "        #author, citation_count = line_content[0].split(\"\\t\")\n",
    "        if author in author_ids.keys():\n",
    "            author_citations[author] = int(citation_count)\n",
    "\n",
    "    # Paper and citation counts\n",
    "\n",
    "    path_paper_citations = \"data/\" + str(year) + \"/paper_citations.txt\"\n",
    "\n",
    "    f = open(path_paper_citations, 'r', encoding=\"Latin-1\")\n",
    "    paper_citations = {}\n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        splitedParts = line_content[0].split(\"\\t\")\n",
    "        if len(splitedParts) >= 2:\n",
    "            citation_count, paper = splitedParts[0], splitedParts[1]\n",
    "            #paper, citation_count = splitedParts[0], splitedParts[1]\n",
    "            paper_citations[paper] = int(citation_count)\n",
    "\n",
    "    \n",
    "    path_author_collaboration_network = \"data/\" + str(year) + \"/networks/author-collaboration-network.txt\"\n",
    "    #path_author_collaboration_network = \"data/test.txt\"\n",
    "\n",
    "    f = open(path_author_collaboration_network, 'r', encoding=\"Latin-1\")\n",
    "\n",
    "    author_collaboration_network = nx.Graph()\n",
    "\n",
    "    for v in author_ids.keys():\n",
    "        author_collaboration_network.add_node(v)\n",
    "    for line in f:\n",
    "        sumcitation = 0\n",
    "        line_content = line.split(\"\\n\")\n",
    "        name_author, name_coauthor = line_content[0].split(\" ==> \")\n",
    "        #if name_author in top100_sorted_author_citations.keys() and name_coauthor in top100_sorted_author_citations.keys():\n",
    "        #    author_collaboration_network.add_edge(name_author, name_coauthor)\n",
    "\n",
    "        if name_author in author_ids.keys() and name_coauthor in author_ids.keys():\n",
    "            for key, value in paper_authors.items():\n",
    "                if author_ids[name_author] in value and author_ids[name_coauthor] in value :\n",
    "                    if key in paper_citations.keys():\n",
    "                        sumcitation = sumcitation + paper_citations[key]\n",
    "                    #print (name_author + \" & \" + name_coauthor + \" --> \" + sumcitation)\n",
    "            author_collaboration_network.add_edge(name_author, name_coauthor, weight=sumcitation)\n",
    "            #if sumcitation>50:\n",
    "            #    print (name_author + \" & \" + name_coauthor + \" --> \" + str(sumcitation))\n",
    "\n",
    "    f.close()\n",
    "    outputfile_centrality_eigen = open('data2/author_eigenvector_centrality_weighted_' + str(year) + '.txt', 'w', encoding=\"Latin-1\")\n",
    "\n",
    "    for item in sorted(nx.eigenvector_centrality(author_collaboration_network, weight = 'weight').items(), key=operator.itemgetter(1), reverse=True):\n",
    "        outputfile_centrality_eigen.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "    outputfile_centrality_eigen.close()\n",
    "    \n",
    "    outputfile_sorted_citations = open('data/author_citations_sorted_' + str(year) + '.txt', 'w', encoding=\"Latin-1\")\n",
    "    for item in sorted(author_citations.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        outputfile_sorted_citations.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "    outputfile_sorted_citations.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputfile_sorted_citations.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " for year in [2010]:\n",
    "    \n",
    "    # Create the dict of authors with citations\n",
    "\n",
    "    path_author_citations = \"data/\" + str(year) + \"/author_citations.txt\"\n",
    "\n",
    "    f = open(path_author_citations, 'r', encoding=\"Latin-1\")\n",
    "    author_citations = {}\n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        #citation_count, author = line_content[0].split(\"\\t\")\n",
    "        author, citation_count = line_content[0].split(\"\\t\")\n",
    "        if author in author_ids.keys():\n",
    "            author_citations[author] = int(citation_count)\n",
    "\n",
    "    # Paper and citation counts\n",
    "\n",
    "    path_paper_citations = \"data/\" + str(year) + \"/paper_citations.txt\"\n",
    "\n",
    "    f = open(path_paper_citations, 'r', encoding=\"Latin-1\")\n",
    "    paper_citations = {}\n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        splitedParts = line_content[0].split(\"\\t\")\n",
    "        if len(splitedParts) >= 2:\n",
    "            citation_count, paper = splitedParts[0], splitedParts[1]\n",
    "            #paper, citation_count = splitedParts[0], splitedParts[1]\n",
    "            paper_citations[paper] = int(citation_count)\n",
    "\n",
    "    \n",
    "    path_author_collaboration_network = \"data/\" + str(year) + \"/networks/author-collaboration-network.txt\"\n",
    "    #path_author_collaboration_network = \"data/test.txt\"\n",
    "\n",
    "    f = open(path_author_collaboration_network, 'r', encoding=\"Latin-1\")\n",
    "\n",
    "    author_collaboration_network = nx.Graph()\n",
    "\n",
    "    for v in author_ids.keys():\n",
    "        author_collaboration_network.add_node(v)\n",
    "    for line in f:\n",
    "        sumcitation = 0\n",
    "        line_content = line.split(\"\\n\")\n",
    "        name_author, name_coauthor = line_content[0].split(\" ==> \")\n",
    "        #if name_author in top100_sorted_author_citations.keys() and name_coauthor in top100_sorted_author_citations.keys():\n",
    "        #    author_collaboration_network.add_edge(name_author, name_coauthor)\n",
    "\n",
    "        if name_author in author_ids.keys() and name_coauthor in author_ids.keys():\n",
    "            for key, value in paper_authors.items():\n",
    "                if author_ids[name_author] in value and author_ids[name_coauthor] in value :\n",
    "                    if key in paper_citations.keys():\n",
    "                        sumcitation = sumcitation + paper_citations[key]\n",
    "                    #print (name_author + \" & \" + name_coauthor + \" --> \" + sumcitation)\n",
    "            author_collaboration_network.add_edge(name_author, name_coauthor, weight=sumcitation)\n",
    "            #if sumcitation>50:\n",
    "            #    print (name_author + \" & \" + name_coauthor + \" --> \" + str(sumcitation))\n",
    "\n",
    "    f.close()\n",
    "    outputfile_centrality_eigen = open('data2/author_eigenvector_centrality_weighted_' + str(year) + '.txt', 'w', encoding=\"Latin-1\")\n",
    "\n",
    "    for item in sorted(nx.eigenvector_centrality(author_collaboration_network, weight = 'weight').items(), key=operator.itemgetter(1), reverse=True):\n",
    "        outputfile_centrality_eigen.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "    outputfile_centrality_eigen.close()\n",
    "    \n",
    "    #outputfile_sorted_citations = open('data/author_citations_sorted_' + str(year) + '.txt', 'w', encoding=\"Latin-1\")\n",
    "    #for item in sorted(author_citations.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    #    outputfile_sorted_citations.write(item[0]+\"\\t\"+str(item[1])+\"\\n\")\n",
    "    #outputfile_sorted_citations.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04583912215591 2008\n",
      "3.0236623150276283 2009\n",
      "3.100392766524963 2010\n",
      "4.195181349809288 2011\n",
      "4.087255627570475 2012\n"
     ]
    }
   ],
   "source": [
    "for year in [2008,2009,2010,2011,2012]:\n",
    "    \n",
    "    # Create the dict of authors with citations\n",
    "\n",
    "    path_author_centrality = 'data2/author_eigenvector_centrality_weighted_' + str(year) + '.txt'\n",
    "\n",
    "    f = open(path_author_centrality, 'r', encoding=\"Latin-1\")\n",
    "    isitone = 0\n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        splitedParts = line_content[0].split(\"\\t\")\n",
    "        name, citation_count = splitedParts[0], splitedParts[1]\n",
    "        isitone = isitone + float(citation_count)\n",
    "    \n",
    "    print(str(isitone) + \" \" + str(year))\n",
    "    f.close()\n",
    "    \n",
    "    outputfile_centrality_eigen = open('data2/author_eigenvector_centrality_weighted_' + str(year) + '_normalized.txt', 'w', encoding=\"Latin-1\")\n",
    "    f = open(path_author_centrality, 'r', encoding=\"Latin-1\")\n",
    "    \n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        splitedParts = line_content[0].split(\"\\t\")\n",
    "        name, citation_count = splitedParts[0], splitedParts[1]\n",
    "        outputfile_centrality_eigen.write(name+\"\\t\"+str(float(citation_count)/isitone)+\"\\n\")\n",
    "    outputfile_centrality_eigen.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999989 2008\n",
      "1.0000000000000098 2009\n",
      "0.9999999999999999 2010\n",
      "1.0 2011\n",
      "0.9999999999999974 2012\n"
     ]
    }
   ],
   "source": [
    "for year in [2008,2009,2010,2011,2012]:\n",
    "    path_author_centrality_normalized = 'data2/author_eigenvector_centrality_weighted_' + str(year) + '_normalized.txt'\n",
    "    f = open(path_author_centrality_normalized, 'r', encoding=\"Latin-1\")\n",
    "    isitone = 0\n",
    "    for line in f:\n",
    "        line_content = line.split(\"\\n\")\n",
    "        splitedParts = line_content[0].split(\"\\t\")\n",
    "        name, citation_count = splitedParts[0], splitedParts[1]\n",
    "        isitone = isitone + float(citation_count)\n",
    "\n",
    "    print(str(isitone) + \" \" + str(year))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getWeightedAverage(list):\n",
    "    if len(list) <= 1:\n",
    "        return list[0]\n",
    "    else:\n",
    "        return list[0]*0.5 + (0.5/(len(list)-1)) * sum(list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.5\n"
     ]
    }
   ],
   "source": [
    "print(getWeightedAverage([100,80,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
