{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10434\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "path_titles_meta = \"acl-metadata.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f = open(path_titles_meta, \"r\", encoding=\"Latin-1\")\n",
    "\n",
    "# compile titles into a list\n",
    "titles = []\n",
    "titlesDict = {}\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "#Extracting the titles from our meta-document\n",
    "for line in f:\n",
    "    if line.startswith('title'):\n",
    "        tmp = line.split(' = ',2)\n",
    "        title = tmp[1][1:len(tmp[1])-2] #last part: subtracts /n}; \n",
    "        titles.append(title)\n",
    "        titlesDict[title] = None\n",
    "\n",
    "\n",
    "\n",
    "# loop through title list    \n",
    "for i in titles:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    #show tokenized words\n",
    "    #print(tokens)\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "   \n",
    "    \n",
    "    # stem token \"-e are lost??\" \n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    #print(stemmed_tokens)\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stopped_tokens)\n",
    "    \n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(max([v for s in corpus for v, _ in s]))\n",
    "\n",
    "# generate LDA model\n",
    "#ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "\n",
    "#print(ldamodel.print_topics(num_topics=5, num_words=5))\n",
    "\n",
    "\n",
    "#print(len(titlesDict.keys()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19424, 10435)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "dictionary_len = max([v for s in corpus for v, _ in s]) + 1\n",
    "document_len = len(corpus)\n",
    "converted_input = csr_matrix((document_len, dictionary_len), dtype=np.int16).toarray()\n",
    "print(converted_input.shape)\n",
    "tup_dict = corpus\n",
    "\n",
    "for i in range(len(tup_dict)):\n",
    "    for (word_idx, count) in tup_dict[i]:\n",
    "        converted_input [i, word_idx] = count\n",
    "        \n",
    "corpus = converted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for i in range(len(dictionary.keys())):\n",
    "    vocab.append(dictionary.get(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: translation machine statistical based system phrase state\n",
      "Topic 1: extraction using based information entity learning text\n",
      "Topic 2: resolution task system coreference annotation evaluation english\n",
      "Topic 3: semantic discourse role using learning expressions relations\n",
      "Topic 4: evaluation automatic text annotation using detection error\n",
      "Topic 5: speech word based segmentation chinese recognition tagging\n",
      "Topic 6: parsing dependency grammars grammar based tree using\n",
      "Topic 7: dialogue system language spoken systems speech question\n",
      "Topic 8: language natural processing computational grammar generation semantics\n",
      "Topic 9: word sense using semantic corpora disambiguation lexical\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "\n",
    "model = lda.LDA(n_topics=10, n_iter=500, random_state=1)\n",
    "model.fit(corpus)\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls = model.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing 1.0\n"
     ]
    }
   ],
   "source": [
    "print(str(titles[0]) + \" \" + str(sum(cls[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.90432999e-04   6.39876055e-04   7.07266215e-04 ...,   4.92531463e-03\n",
      "    3.96640773e-01   3.36868867e-05]\n",
      " [  3.05528806e-03   4.51801220e-03   2.15391510e-05 ...,   2.04331320e-03\n",
      "    3.81994719e-04   6.97641261e-03]\n",
      " [  8.42137999e-03   3.08472390e-02   1.53655111e-03 ...,   3.59701412e-01\n",
      "    9.08223080e-04   5.96388551e-02]\n",
      " ..., \n",
      " [  5.07129135e-01   7.61880853e-04   1.25600907e-02 ...,   3.71346724e-02\n",
      "    3.29966240e-02   3.86706593e-03]\n",
      " [  5.30792150e-01   1.46837784e-04   1.44271382e-02 ...,   1.19263660e-02\n",
      "    3.89206178e-02   7.79009494e-02]\n",
      " [  6.08861657e-01   2.43025257e-03   2.92429517e-04 ...,   6.44618565e-04\n",
      "    1.31408335e-04   7.21876738e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_titles_meta_lda = \"acl-metadata-lda.txt\"\n",
    "\n",
    "\n",
    "f = open(path_titles_meta_lda, \"w\", encoding=\"Latin-1\")\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    f.write(str(titles[i]) + \"\\t\" + str(cls[i]) + \"\\n\")\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
